---
title: "linear.regression"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{linear_regression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE,
                      comment = "#>")
```

```{r setup}
library(linear.regression)
```

# Introduction


In statistics, `linear.regression` is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). `linear.regression` allows users to: 

1. Fit linear regression model with or without intercept and obtain the estimated coeficients. 
2. Obtain estimated y's.
3. Obtain sum of squares. 
4. Obtain variances of estimators.
5. Obtain the t and f statistics and p values.
6. Obtain the hat matrix. 
7. Obtain residuals. 


## Install the package
`linear.regression` is a package for R, so you need [R](https://cloud.r-project.org/) first. We also strongly recommend the [RStudio](https://www.rstudio.com/products/rstudio/download/) integrated development environment as a user-friendly graphical wrapper for R.

`linear.regression` is now available on github. In order to install a package from GitHub, you will need the `devtools::` package (https://github.com/r-lib/devtools) and either [Rtools](https://cran.r-project.org/bin/windows/Rtools/) (for Windows) or [Xcode](https://developer.apple.com/xcode/) (for Mac). The following is the code for installing the package on Rstudio:
```
devtools::install_github("BangyaoZhao/linear.regression",build_vignettes = T)
```
## Loading packages
`linear regression` can be loaded to R using
```{r}
library(linear.regression)
```

# How to use

## Obtain coefficients
Instead of a ~ expression that the `stats::lm()` function uses, `linear.regression` is designed to be more user friendly. It requires a outcome vector(could be either a vector or a n by 1 matrix) to be the first argument, and a covariates matrix(should not include the intercept) to be the second argument. `linear_regression` will return the user a list including all the results. 

Before the example, let's load the `mtcars` data adapted for this package.
```{r}
data("mtcars_mpg")
mtcars_mpg[1:3, ]
data("mtcars_covariates")
mtcars_covariates[1:3, ]
```

Also, you can see the documentations to know more:
```
help(mtcars_mpg)
help(mtcars_covariates)
```


Then, fit the regression model by `linear_regression` function

```{r}
linear_regression(mtcars_mpg, mtcars_covariates)
```

Let's compare the result with the `stats::lm` function

```{r}
lm(mtcars_mpg ~ mtcars_covariates)
```

Just like `stats::lm` function, the `linear_regression` function has an intercept by default. If the user does not want the intercept, `intercept = F` could be used. 

```{r}
linear_regression(mtcars_mpg, mtcars_covariates, intercept = F)
lm(mtcars_mpg ~ 0 + mtcars_covariates)
```

## Obtain estimated y's

In cases where the user want to have estimated y's in the result, `y_bars = T` could be used. 

```{r}
linear_regression(mtcars_mpg, mtcars_covariates, y_bars = T)$y_bars[1:5, ]
lm(mtcars_mpg ~ mtcars_covariates)$"fitted.values"[1:5]
```

## Obtain sum of squares. 

In statistic analysis, sum of squares ,their degrees of freedom and R_sqare value are very important. If want, the user can set `ss = T`.

```{r}
linear_regression(mtcars_mpg, mtcars_covariates, ss = T)$ss
res = anova(lm(mtcars_mpg ~ mtcars_covariates))
res[1:2, 1:2]
```

An example without intercept

```{r}
linear_regression(mtcars_mpg,
                  mtcars_covariates,
                  intercept = F,
                  ss = T)$ss
res = anova(lm(mtcars_mpg ~ 0 + mtcars_covariates))
res[1:2, 1:2]
```

## Obtain variances of estimators.
```{r}
linear_regression(mtcars_mpg, mtcars_covariates, variances = T)$variances
```
## Obtain the t and f statistics and p values.

Obtain t statistics and p_values

With intercept:

```{r}
linear_regression(mtcars_mpg, mtcars_covariates, t_test = T)$t_test
summary(lm(mtcars_mpg ~ mtcars_covariates))
```

With out intercept:

```{r}
linear_regression(mtcars_mpg,
                  mtcars_covariates,
                  intercept = F,
                  t_test = T)$t_test
summary(lm(mtcars_mpg ~ 0 + mtcars_covariates))
```

Obtain f statistics and p_values

With intercept:

```{r}
linear_regression(mtcars_mpg, mtcars_covariates, f_test = T)$f_test
anova(lm(mtcars_mpg ~ mtcars_covariates))
```

Without intercept:

```{r}
linear_regression(mtcars_mpg,
                  mtcars_covariates,
                  intercept = F,
                  f_test = T)$f_test
anova(lm(mtcars_mpg ~ 0 + mtcars_covariates))
```

## Obtain the hat matrix. 

Simply by setting `hat_matrix=T`

```{r}
linear_regression(mtcars_mpg, mtcars_covariates, hat_matrix = T)$hat_matrix[1:3, 1:3]
```

## Obtain residuals.

Simply by setting `residuals=T`
```{r}
linear_regression(mtcars_mpg, mtcars_covariates, residuals = T)$residuals[1:5, ]
lm(mtcars_mpg ~ mtcars_covariates)$residuals[1:5]
```

# Efficiency

`linear_regression` is designed in the modern concept of Minimalism, meaning it won't waste computing power on what users didn't request. Therefore, it runs fast than `lm`.

Let's have a look on an example.

```{r}
compare_result = bench::mark({
  res1 = linear_regression(mtcars_mpg, mtcars_covariates)$coefs
  as.vector(res1)
}, {
  res2 = lm(mtcars_mpg ~ mtcars_covariates)$coefficients
  as.vector(res2)
})
compare_result = as.numeric(compare_result[[3]])
ratio = compare_result[2] / compare_result[1]
cat("linear_regression() is ", ratio, " times as fast as lm()")
```









